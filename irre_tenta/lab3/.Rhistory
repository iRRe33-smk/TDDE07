print(summary(posterior_beta_draws[,6]))
# Getting sampled values for the beta paramters for NSmallChild(6th covariate)
small_kids_draw = posterior_beta_draws[,6]
# Finding the 95% equal tail credible interval from the sampled values for NSmallChild and adding it to the plot
q = quantile(small_kids_draw, probs=c(0.025,0.975))
print(q)
q = unname(q)
q_lo = q[1]
q_hi = q[2]
abline(v=q_lo, col="red")
abline(v=q_hi, col="red")
legend("topleft", inset=.02, c("95% equal tailed credible intervel"), fill=c("red"))
# Creating a data point for a women of age 43, with 2 small children, 12 years of education, 8 years experience and with husband income 20
X_test = c(1,20,12, 8, 43, 2, 0)
data.frame(X)
head(data.frame(X))
# Getting predictions that women is working based for the test case created above based on the sampled beta parameters from the posterior of logistic regression
res = X_test %*% t(posterior_beta_draws)
yhat_test = exp(res)/(1+exp(res))
# Plotting the distribution of the predicted values of the test case
plot(main = "Density plot of probablity of the women working", density(yhat_test))
hist(yhat_test, main = "Histogram of Prediction of probablity of the women working")
num_of_working_women <- c()
# looping through all the beta values sampled for posterior
for (ind in 1:dim(posterior_beta_draws)[1]){
# Considering p as the probability of success for 1 women found using the beta parameter
p  = yhat_test[ind]
# Getting number of women working as random draws from the binomial distribution with n = 11 and p = probability of success for one women found using posterior betas
num_of_working_women = c(num_of_working_women, rbinom(1, 11, p))
}
# Plotting posterior predictive distribution of number of women working out of 11
hist(num_of_working_women, freq = FALSE, probability = TRUE, main = "Histogram of probablity of number of out of 11 women working")
plot(density(num_of_working_women), main = "Density plot of probablity of number of out of 11 women working")
# Plotting posterior predictive distribution of number of women working out of 11
hist(num_of_working_women, freq = FALSE, probability = TRUE, main = "Histogram of probablity of number of out of 11 women working")
library(scales)
data = readRDS("Precipitation.rds")
setwd("C:/Users/IsakG/projects/baysian-learning/irre_tenta/lab3")
library(scales)
data = readRDS("Precipitation.rds")
library(scales)
data = readRDS("Precipitation.rds")
y = log(data)
n = length(y)
y_mean = mean(y)
# initializing hyper parameters
#these are not given and no method for choosing provided
#have chosen reasonable values and checked with draws from posterior
mu_0 = mean(y)*1.2
# initializing hyper parameters
#these are not given and no method for choosing provided
#have chosen reasonable values and checked with draws from posterior
mu_0 = mean(y)*1.2
t_sq_0 = var(y)*.8
sigma_sq_0 = .5
v0 = 100*.1
# Plotting prior to check if it is at all similar to the data given
#simulated draws from inverss chi^2
prior_sigmasq = c()
N_Draws = 2000
prior_mu = rnorm(N_Draws,mu_0,t_sq_0)
for(i in 1:N_Draws){
X = rchisq(1,v0)
prior_sigmasq = c(prior_sigmasq, v0*sigma_sq_0 / X)
}
# Drawing form the simulated prior mean and sigma^2
draws_from_prior = c()
for(i in 1:N_Draws){
draws_from_prior = c(draws_from_prior, rnorm(1, prior_mu[i],prior_sigmasq[i]))
}
# Plotting histogram of data points
hist(draws_from_prior)
mean(draws_from_prior)
var(draws_from_prior)
#plotting given data
hist(y)
mean(y)
var(y)
var(y)
N_Draws = n
# initializing hyper parameters
mu_0 = mean(y)*1.2
t_sq_0 = var(y)*.8
sigma_sq_0 = .5
v0 = 100*.1
v_n = v0 + n
mu = rnorm(1,mu_0,t_sq_0) # draw using our hyperparams
#draw from inv_chi^2
X = rchisq(1,v0)
sigmasq = v0*sigma_sq_0 / X
Gibbs_posterior_mu = c()
Gibbs_posterior_sigma = c()
for( i in 1:N_Draws){
w = (n/sigmasq) / ( (n/sigmasq) + (1 / t_sq_0))
mu_n = w*y_mean + (1-w)*mu_0
t_sq_n = 1/((n/sigmasq) + (1/t_sq_0))
mu = rnorm(1,mu_n,t_sq_n)
Gibbs_posterior_mu = c(Gibbs_posterior_mu, mu)
X = rchisq(1,v_n)
thing = (v0*sigma_sq_0 + sum((mu - y)^2))/(n + v0)
sigmasq = v_n*thing/X
Gibbs_posterior_sigma = c(Gibbs_posterior_sigma, sigmasq)
}
thing = (v0*sigma_sq_0 + sum((mu - y)^2))/(n + v0)
for( i in 1:N_Draws){
w = (n/sigmasq) / ( (n/sigmasq) + (1 / t_sq_0))
mu_n = w*y_mean + (1-w)*mu_0
t_sq_n = 1/((n/sigmasq) + (1/t_sq_0))
mu = rnorm(1,mu_n,t_sq_n)
Gibbs_posterior_mu = c(Gibbs_posterior_mu, mu)
X = rchisq(1,v_n)
thing = (v0*sigma_sq_0 + sum((mu - y)^2))/(n + v0)
sigmasq = v_n*thing/X
Gibbs_posterior_sigma = c(Gibbs_posterior_sigma, sigmasq)
}
# initializing hyper parameters
mu_0 = mean(y)*1.2
t_sq_0 = var(y)*.8
sigma_sq_0 = .5
v0 = 100*.1
v_n = v0 + n
# Initial Setting
mu = rnorm(1,mu_0,t_sq_0) # draw using our hyperparams
#draw from inv_chi^2
X = rchisq(1,v0)
sigmasq = v0*sigma_sq_0 / X
Gibbs_posterior_mu = c()
Gibbs_posterior_sigma = c()
#mu is mean of the normal distribution for y
#mu_n is the mean of the normal distribution fro mu
#t_sq_n is the variance of the normal distribution for mu
# v0, v_n and sigma_sq_0 are used in drawing for sigma_sq
for( i in 1:N_Draws){
w = (n/sigmasq) / ( (n/sigmasq) + (1 / t_sq_0))
mu_n = w*y_mean + (1-w)*mu_0
t_sq_n = 1/((n/sigmasq) + (1/t_sq_0))
mu = rnorm(1,mu_n,t_sq_n)
Gibbs_posterior_mu = c(Gibbs_posterior_mu, mu)
X = rchisq(1,v_n)
thing = (v0*sigma_sq_0 + sum((mu - y)^2))/(n + v0)
sigmasq = v_n*thing/X
Gibbs_posterior_sigma = c(Gibbs_posterior_sigma, sigmasq)
}
#autocorrelations of sampled params
acf_mu = acf(Gibbs_posterior_mu)
acf_sigma = acf(Gibbs_posterior_sigma)
#inefficiencty factors
if_mu = 1+2*sum(acf_mu$acf[-1])
if_sigma = 1+2*sum(acf_sigma$acf[-1])
hist(Gibbs_posterior_mu)
hist(Gibbs_posterior_sigma)
plot(Gibbs_posterior_mu,type="l")
plot(Gibbs_posterior_sigma,type="l")
#--------------verification, not part of question -----------------------
# Direct Draws Done for verification
mu_0 = mean(y)
k0 = 1
sigma_sq_0 = 1.8
v0 = 100
# Draws from Piror
nDraws = 2000
X = rchisq(nDraws,v0)
prior_direct_sigmasq = v0*sigma_sq_0 / X
prior_direct_mu = c()
for(i in 1:nDraws){
prior_direct_mu = c(prior_direct_mu, rnorm(1,mu_0,prior_direct_sigmasq[i]/k0))
}
hist(mu)
hist(prior_direct_sigmasq)
# Posterior from Direct Sampling
nDraws = 500
mu_n = ((k0/(k0+n))*mu_0) + ((n/(k0+n))*y_mean)
kn = k0 + n
vn = v0 + n
s_sq = sum((y-y_mean)^2)/(n-1)
sigmasq_n = (v0*sigma_sq_0 + (n-1)*s_sq + (((k0*n)/(k0+n))*(y_mean - mu_0)^2))/vn
X = rchisq(nDraws,vn)
direct_sigmasq = vn*sigmasq_n / X
direct_mu = c()
for(i in 1:nDraws){
direct_mu = c(direct_mu, rnorm(1,mu_n,direct_sigmasq[i]/kn))
}
hist(direct_mu)
hist(direct_sigmasq)
plot(direct_mu,type="l")
plot(direct_sigmasq,type="l")
hist(data)
hist(data)
hist(data)
draws_posterior = c()
for(i in 1:n){
draws_posterior = c(draws_posterior, rnorm(1,Gibbs_posterior_mu[i], Gibbs_posterior_sigma[i]))
}
hist(exp(draws_posterior), breaks = 200, xlim=range(0,80))
a = hist(data)
draws_posterior = c()
for(i in 1:n){
draws_posterior = c(draws_posterior, rnorm(1,Gibbs_posterior_mu[i], Gibbs_posterior_sigma[i]))
}
b = hist(exp(draws_posterior), breaks = 200, xlim=range(0,80))
plot(a, col="red")
plot(b, col="blue")
hist(data)
hist(exp(draws_posterior), breaks = 200, xlim=range(0,80))
setwd("C:/Users/IsakG/projects/baysian-learning/irre_tenta/lab3")
library(mvtnorm)
data <- read.table("eBayNumberOfBidderData.dat", sep = "" , header = T , na.strings ="", stringsAsFactors= F)
data = as.data.frame(data)
col_names = colnames(data)
# lab 3.2.1
model <- glm(nBids ~ . , data[,-2], family = poisson(link = "log"))
model
summary(model)
# lab 3.2.2
Y = data$nBids
X = as.matrix(data[,-1])
n = dim(data)[1]
mu = rep(0,9)
Sigma = 100*solve(t(X)%*%X)
beta_prior = rmvnorm(1,mean=mu,sigma=Sigma)
dim(beta_prior)
LogPostPoisson <- function(betas,Y,X,mu,Sigma){
linPred <- betas%*%t(X);
# finding the log likelihood
logLik <- sum(Y*linPred - exp(linPred) - log(factorial(Y)));
#print(logLik)
# finding the log logPrior using the parameters given
logPrior <- dmvnorm(betas, mu, Sigma, log=TRUE);
#print(logPrior)
# returning the log posterior as the sum of loglikihood and logPrior
return(logLik + logPrior)
}
OptimRes <- optim(beta_prior,LogPostPoisson,gr=NULL,Y,X,mu,Sigma,method=c("BFGS"),control=list(fnscale=-1),hessian=TRUE)
# Getting the hessian matrix from the results returned from the optim function
beta_mode_hessian = OptimRes$hessian
# Getting the mode Î² values as the parameters returned from the optim function
beta_mode = OptimRes$par
colnames(beta_mode) = col_names[-1]
# Getting the J(beta_mode) as the negative of the hessian returned
J = -beta_mode_hessian
# Calculating inverse of J
J_inverse = solve(-beta_mode_hessian)
posterior_beta_draw = rmvnorm(1,beta_mode, J_inverse)
posterior_beta_draw
# lab 3.2.3
num_random_walk = 1E5
for(i in 1:num_random_walk){
sample = rmvnorm(1, mean = prev_beta, sigma = c * J_inverse)
log_density_sample = log_density_function(sample, Y, X, mu, Sigma)
log_density_prev_sample = log_density_function(prev_beta, Y, X, mu, Sigma)
alpha = min(1, exp((log_density_sample - log_density_prev_sample)))
rand = runif(n=1, min = 0, max = 1)
if(rand<alpha){
metropolis_betas[i,] = sample
prev_beta = sample
}
else{
metropolis_betas[i,] = prev_beta
}
}
library(mvtnorm)
data <- read.table("eBayNumberOfBidderData.dat", sep = "" , header = T , na.strings ="", stringsAsFactors= F)
data = as.data.frame(data)
col_names = colnames(data)
# lab 3.2.1
model <- glm(nBids ~ . , data[,-2], family = poisson(link = "log"))
model
summary(model)
# lab 3.2.2
Y = data$nBids
X = as.matrix(data[,-1])
n = dim(data)[1]
mu = rep(0,9)
Sigma = 100*solve(t(X)%*%X)
beta_prior = rmvnorm(1,mean=mu,sigma=Sigma)
dim(beta_prior)
LogPostPoisson <- function(betas,Y,X,mu,Sigma){
linPred <- betas%*%t(X);
# finding the log likelihood
logLik <- sum(Y*linPred - exp(linPred) - log(factorial(Y)));
#print(logLik)
# finding the log logPrior using the parameters given
logPrior <- dmvnorm(betas, mu, Sigma, log=TRUE);
#print(logPrior)
# returning the log posterior as the sum of loglikihood and logPrior
return(logLik + logPrior)
}
OptimRes <- optim(beta_prior,LogPostPoisson,gr=NULL,Y,X,mu,Sigma,method=c("BFGS"),control=list(fnscale=-1),hessian=TRUE)
# Getting the hessian matrix from the results returned from the optim function
beta_mode_hessian = OptimRes$hessian
# Getting the mode Î² values as the parameters returned from the optim function
beta_mode = OptimRes$par
colnames(beta_mode) = col_names[-1]
# Getting the J(beta_mode) as the negative of the hessian returned
J = -beta_mode_hessian
# Calculating inverse of J
J_inverse = solve(-beta_mode_hessian)
posterior_beta_draw = rmvnorm(1,beta_mode, J_inverse)
# lab 3.2.3
num_random_walk = 1E5
MetropolisRandomWalk <- function(beta_initial, c, J_inverse, log_density_function, mu, Sigma, Y, X){
metropolis_betas = matrix(data = NA, nrow = num_random_walk, ncol = dim(X)[2])
prev_beta = beta_initial
for(i in 1:num_random_walk){
sample = rmvnorm(1, mean = prev_beta, sigma = c * J_inverse)
log_density_sample = log_density_function(sample, Y, X, mu, Sigma)
log_density_prev_sample = log_density_function(prev_beta, Y, X, mu, Sigma)
alpha = min(1, exp((log_density_sample - log_density_prev_sample)))
rand = runif(n=1, min = 0, max = 1)
if(rand<alpha){
metropolis_betas[i,] = sample
prev_beta = sample
}
else{
metropolis_betas[i,] = prev_beta
}
}
return(metropolis_betas)
}
metropolis_posterior_betas = MetropolisRandomWalk(posterior_beta_draw, .6, J_inverse, LogPostPoisson, mu, Sigma, Y, X)
colnames(metropolis_posterior_betas) = col_names[-1]
par(mfrow=c(3,3))
for(i in 1:dim(X)[2]){
plot(metropolis_posterior_betas[,i], type = "l", ylim = c(0,2*model$coefficients[i]),ylab = col_names[1+i])
abline(h=model$coefficients[i], col="red")
}
par(mfrow=c(1,1))
library(mvtnorm)
data <- read.table("eBayNumberOfBidderData.dat", sep = "" , header = T , na.strings ="", stringsAsFactors= F)
data = as.data.frame(data)
col_names = colnames(data)
# lab 3.2.1
model <- glm(nBids ~ . , data[,-2], family = poisson(link = "log"))
model
summary(model)
# lab 3.2.2
Y = data$nBids
X = as.matrix(data[,-1])
n = dim(data)[1]
mu = rep(0,9)
Sigma = 100*solve(t(X)%*%X)
beta_prior = rmvnorm(1,mean=mu,sigma=Sigma)
dim(beta_prior)
LogPostPoisson <- function(betas,Y,X,mu,Sigma){
linPred <- betas%*%t(X);
# finding the log likelihood
logLik <- sum(Y*linPred - exp(linPred) - log(factorial(Y)));
#print(logLik)
# finding the log logPrior using the parameters given
logPrior <- dmvnorm(betas, mu, Sigma, log=TRUE);
#print(logPrior)
# returning the log posterior as the sum of loglikihood and logPrior
return(logLik + logPrior)
}
OptimRes <- optim(beta_prior,LogPostPoisson,gr=NULL,Y,X,mu,Sigma,method=c("BFGS"),control=list(fnscale=-1),hessian=TRUE)
# Getting the hessian matrix from the results returned from the optim function
beta_mode_hessian = OptimRes$hessian
# Getting the mode Î² values as the parameters returned from the optim function
beta_mode = OptimRes$par
colnames(beta_mode) = col_names[-1]
# Getting the J(beta_mode) as the negative of the hessian returned
J = -beta_mode_hessian
# Calculating inverse of J
J_inverse = solve(-beta_mode_hessian)
posterior_beta_draw = rmvnorm(1,beta_mode, J_inverse)
# lab 3.2.3
num_random_walk = 1E5
MetropolisRandomWalk <- function(beta_initial, c, J_inverse, log_density_function, mu, Sigma, Y, X){
metropolis_betas = matrix(data = NA, nrow = num_random_walk, ncol = dim(X)[2])
prev_beta = beta_initial
for(i in 1:num_random_walk){
sample = rmvnorm(1, mean = prev_beta, sigma = c * J_inverse)
log_density_sample = log_density_function(sample, Y, X, mu, Sigma)
log_density_prev_sample = log_density_function(prev_beta, Y, X, mu, Sigma)
alpha = min(1, exp((log_density_sample - log_density_prev_sample)))
rand = runif(n=1, min = 0, max = 1)
if(rand<alpha){
metropolis_betas[i,] = sample
prev_beta = sample
}
else{
metropolis_betas[i,] = prev_beta
}
}
return(metropolis_betas)
}
metropolis_posterior_betas = MetropolisRandomWalk(posterior_beta_draw, .6, J_inverse, LogPostPoisson, mu, Sigma, Y, X)
colnames(metropolis_posterior_betas) = col_names[-1]
par(mfrow=c(3,3))
for(i in 1:dim(X)[2]){
plot(metropolis_posterior_betas[,i], type = "l", ylim = c(0,2*model$coefficients[i]),ylab = col_names[1+i])
abline(h=model$coefficients[i], col="red")
}
par(mfrow=c(1,1))
sample = matrix(c(1, 1, 0, 1, 0, 1,0,1.2,0.8))
prediction = exp(metropolis_posterior_betas %*% sample)
print(exp(model$coefficients%*%sample))
hist(prediction, main = "Predictive distribution", sub =paste("glm predicts: ",exp(model$coefficients%*%sample)))
prob_no_bids = mean(rpois(num_random_walk,prediction)==0)
prob_no_bids
38+27+35
pbeta(.4,shape1 = (a+choice_a), shape2 = (a+b+n))
#-------------------1.a-------------------
#prior
a = 16
b = 24
n=100
choice_a = 38
choice_b = 27
choice_c = 35
#posterior
N = 1E5
#random_draws = rbeta(N,shape1 = (a+choice_a), shape2 = (a+b+n))
pbeta(.4,shape1 = (a+choice_a), shape2 = (a+b+n))
#-------------------1.a-------------------
#prior
a = 16
b = 24
n=100
choice_a = 38
choice_b = 27
choice_c = 35
#posterior
N = 1E5
#random_draws = rbeta(N,shape1 = (a+choice_a), shape2 = (a+b+n))
prob_less_than = pbeta(.4,shape1 = (a+choice_a), shape2 = (a+b+n))
print(prob_less_than)
(a+b) / ( a+b+n)
plot(1-random_draws)
N = 1E5
random_draws = rbeta(N,shape1 = (a+choice_a), shape2 = (a+b+n))
plot(1-random_draws)
hist(1-random_draws,probs=TRUE)
hist(1-random_draws,probability=TRUE)
hist(1-random_draws,probability=TRUE, main = "probability distribution of 1-Theta_a")
hist(1-random_draws,probability=TRUE, main = "probability distribution of 1-Theta_a", sub = paste("prob theta_a < ",val, " = ",prob_less_than,sep=""))
#posterior
val = .4
#-------------------1.a-------------------
#prior
a = 16
b = 24
n=100
choice_a = 38
choice_b = 27
choice_c = 35
#posterior
val = .4
prob_less_than = pbeta(val,shape1 = (a+choice_a), shape2 = (a+b+n))
print(prob_less_than)
N = 1E5
random_draws = rbeta(N,shape1 = (a+choice_a), shape2 = (a+b+n))
hist(1-random_draws,probability=TRUE, main = "probability distribution of 1-Theta_a", sub = paste("prob theta_a < ",val, " = ",prob_less_than,sep=""))
points(x=random_draws,y=rat)
rat = (1-random_draws)/random_draws
points(x=random_draws,y=rat)
plot(x=random_draws,y=rat)
plot(x=random_draws,y=rat)
#-------------------1.a-------------------
#prior
a = 16
b = 24
n=100
choice_a = 38
choice_b = 27
choice_c = 35
#posterior
val = .4
prob_less_than = pbeta(val,shape1 = (a+choice_a), shape2 = (a+b+n))
print(prob_less_than)
N = 1E3
random_draws = rbeta(N,shape1 = (a+choice_a), shape2 = (a+b+n))
hist(1-random_draws,probability=TRUE, main = "probability distribution of 1-Theta_a", sub = paste("prob theta_a < ",val, " = ",prob_less_than,sep=""))
rat = (1-random_draws)/random_draws
#-------------------1.a-------------------
#prior
a = 16
b = 24
n=100
choice_a = 38
choice_b = 27
choice_c = 35
#posterior
val = .4
prob_less_than = pbeta(val,shape1 = (a+choice_a), shape2 = (a+b+n))
print(prob_less_than)
N = 1E3
random_draws = rbeta(N,shape1 = (a+choice_a), shape2 = (a+b+n))
hist(1-random_draws,probability=TRUE, main = "probability distribution of 1-Theta_a", sub = paste("prob theta_a < ",val, " = ",prob_less_than,sep=""))
#-----------------------------1.b------------------
rat = (1-random_draws)/random_draws
plot(x=random_draws,y=rat)
rat
hist(rat)
hist(random_draws)
hist(rat)
abline(v=quantile(rat,c(0.025,0,975)))
abline(v=quantile(x=rat,probs=c(0.025,0,975)))
abline(v=quantile(x=rat,probs=c(0.025,0.975)))
abline(v=q)
q=quantile(x=rat,probs=c(0.025,0.975))
abline(v=q)
abline(v=q, col="red")
1/(1+rat)==random_draws
1/(1+q)
quantile(random_draws,probs=c(0.025,0.975))
q
pbinom(q = 38,1,prob = random_draws,lower.tail = TRUE)
pbinom(q = 38,1,prob = random_draws,lower.tail = FALSE)
pbinom(q = 38,1,prob = mode(random_draws),lower.tail = FALSE)
mode(rat)
(rat)
